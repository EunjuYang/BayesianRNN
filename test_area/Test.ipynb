{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Import ####\n",
    "import subprocess\n",
    "import reader\n",
    "import tensorflow as tf\n",
    "\n",
    "#For Bayesian LSTM cell implementation\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple, MultiRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Functions ####\n",
    "\n",
    "def sample_random_normal(name, mean, std, shape):\n",
    "    \n",
    "    with tf.variable_scope(\"sample_random_normal\"):\n",
    "    \n",
    "        #Inverse softplus (positive std)\n",
    "        standard_dev = tf.log(tf.exp(std) - 1.0) * tf.ones(shape)\n",
    "        \n",
    "        mean = tf.get_variable(name + \"_mean\", initializer=mean, dtype=tf.float32)\n",
    "        standard_deviation = tf.get_variable(name + \"std\", initializer=std, dtype=tf.float32)\n",
    "        #Revert back to std\n",
    "        standard_deviation = tf.nn.softplus(standard_deviation)\n",
    "    \n",
    "        #Sample standard normal\n",
    "        epsilon = tf.random_normal(mean=0, stddev=1, name=\"epsilon\", shape=shape, dtype=tf.float32)\n",
    "      \n",
    "        random_var = mean + standard_deviation*epsilon\n",
    "    \n",
    "    return random_var\n",
    "\n",
    "\n",
    "#### Borrowed from PTB model ####\n",
    "\n",
    "class PTBInput(object):\n",
    "  \"\"\"The input data.\"\"\"\n",
    "\n",
    "  def __init__(self, config, data, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    #self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    self.input_data, self.targets = reader.ptb_producer(\n",
    "        data, batch_size, num_steps, name=name)\n",
    "    \n",
    "\n",
    "    \n",
    "class TestConfig(object):\n",
    "  \"\"\"Tiny config, for testing.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 1\n",
    "  num_layers = 1\n",
    "  num_steps = 1\n",
    "  hidden_size = 2\n",
    "  max_epoch = 1\n",
    "  max_max_epoch = 1\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "  #rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "#### End of borrowed from PTB model ####\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "    Bayesian LSTM Cell framework which inherits from tensorflows BasicLSTMCell\n",
    "    \n",
    "\"\"\"\n",
    "class BayesianLSTMCell(BasicLSTMCell):\n",
    "    def __init__(self,\n",
    "                 mean, #mean for sampling weights\n",
    "                 std, #standard deviation for sampling weights\n",
    "                 # --- below is inherited from BasicLSTMCell --- #\n",
    "                 num_units, #The number of hidden units in the cell (4*num_units)\n",
    "                 forget_bias = 1.0, #bias added to forget gates\n",
    "                 input_size = None, #Deprecated and unused\n",
    "                 state_is_tuple = True, #If True, accepted and returned states are 2-tuples of the c_state and h_state\n",
    "                 activation = tf.tanh):\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.Weights = None\n",
    "        self.Biases = None\n",
    "        \n",
    "        #From BasicLSTMCell\n",
    "        super(BayesianLSTMCell, self).__init__(num_units, forget_bias, input_size, state_is_tuple, activation)\n",
    "    \n",
    "    #Sample weights\n",
    "    def get_weights(self):\n",
    "        with tf.variable_scope(\"CellWeights\"):\n",
    "            self.Weights = sample_random_normal(\"WeightMatrix\", self.mean, self.std, shape = [2*self.num_units,4*self.num_units]) #change shape to variables!\n",
    "            return self.Weights\n",
    "    \n",
    "    #Sample biases\n",
    "    def get_biases(self):\n",
    "        with tf.variable_scope(\"CellBiases\"):\n",
    "            self.Biases = sample_random_normal(\"BiasVector\", self.mean, self.std, shape = [4*self.num_units]) #change shape to variable!\n",
    "            return self.Biases\n",
    "    \n",
    "    #Object call function\n",
    "    def __call__(self, inputs, state):\n",
    "        with tf.variable_scope(\"BayesLSTMCell\"):  # \"BasicLSTMCell\"\n",
    "            \n",
    "            #State is a tuple with the current cell and hidden state vectors\n",
    "            cell, hidden = state\n",
    "            \n",
    "            all_inputs = tf.concat([inputs, hidden], 1)\n",
    "            \n",
    "            Weights = self.get_weights()\n",
    "            Biases = self.get_biases()\n",
    "            \n",
    "            # Wx + b\n",
    "            # For hidden_units = 2, W should be a 8x4 matrix and bias = 8 long vector.\n",
    "            # since previous hidden state h = 2 and current input = 2\n",
    "            #\n",
    "            concat =  tf.nn.bias_add(tf.matmul(all_inputs, Weights), Biases)\n",
    "\n",
    "            #Split data up for the 4 gates\n",
    "            #\n",
    "            #Justify the splitting below. What is actually happening?\n",
    "            #\n",
    "            #i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = tf.split(concat, axis = 1, num_or_size_splits = 4)\n",
    "\n",
    "            #Calculate new cell and hidden states. Calculations are as in Zaremba et al 2015\n",
    "            new_cell = (cell * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i)*self._activation(j))\n",
    "            new_hidden = self._activation(new_cell) * tf.sigmoid(o)\n",
    "            \n",
    "            #Create tuple of the new state\n",
    "            new_state = LSTMStateTuple(new_cell, new_hidden)\n",
    "\n",
    "            return new_hidden, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x1f4074ad320>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiate Tensorboard\n",
    "\n",
    "subprocess.Popen([\"tensorboard\",\"--logdir=tensorboard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Process data ####\n",
    "##\n",
    "## Currently we're only interested in running a minimal sample\n",
    "## To verify graphs/sessions and such\n",
    "##\n",
    "####\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "### Set initial parameters\n",
    "\n",
    "#Current data path\n",
    "path = \"../data/\"\n",
    "\n",
    "### Load data\n",
    "raw_data = reader.ptb_raw_data(path)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "config = TestConfig()\n",
    "\n",
    "train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "\n",
    "#collect input data and targets by calling the ptb_producer function\n",
    "#input_data, targets = reader.ptb_producer(train_data, batch_size, num_steps, name = \"TrainInput\")\n",
    "\n",
    "#Embed the data: Embedding format is [vocab_size, hidden_size] = [20, 2]\n",
    "embedding = tf.get_variable(\"embedding\", [config.vocab_size, config.hidden_size], dtype=tf.float32)\n",
    "#inputs = [20, 1, 2] tensor containing 20 words for one timestep with 2 hidden units\n",
    "input = tf.nn.embedding_lookup(embedding, train_input.input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<__main__.BayesianLSTMCell object at 0x000001F40456B2E8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'CellWeights/sample_random_normal/add:0' shape=(4, 8) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Build the graph ####\n",
    "\n",
    "#Just setting some values for testing implementation\n",
    "\n",
    "cell = BayesianLSTMCell(mean=0.0, std=1.0, num_units=config.hidden_size, state_is_tuple=True)\n",
    "#cell = MultiRNNCell([cell]*2, state_is_tuple=True)\n",
    "#cell = (BasicLSTMCell(num_units=config.hidden_size))\n",
    "\n",
    "#print(cell.state_size)\n",
    "#c = cell.zero_state(config.batch_size, tf.float32)\n",
    "#h = cell.zero_state(config.batch_size, tf.float32)\n",
    "#state = (c,h)\n",
    "#print(state)\n",
    "\n",
    "#print(input[:,0,:])\n",
    "\n",
    "#state = cell.zero_state(batch_size=config.batch_size, dtype=tf.float32)\n",
    "\n",
    "#cell(inputs=input[:,0,:], state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run a session on the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #Create event file for tensorboard\n",
    "    summary_write = tf.summary.FileWriter((\"tensorboard\"),sess.graph)\n",
    "    \n",
    "    #Feed data to a placeholder\n",
    "    #feed_dict = {}\n",
    "    #Run desired stuff\n",
    "    #Y_out = sess.run(Y, feed_dict = feed_dict)\n",
    "    \n",
    "#print(Y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
