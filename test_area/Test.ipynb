{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
=======
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
   "outputs": [],
   "source": [
    "#### Import ####\n",
    "import subprocess\n",
    "import reader\n",
    "import tensorflow as tf\n",
    "\n",
    "#For Bayesian LSTM cell implementation\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple, MultiRNNCell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_input contains a pair of Tensors (.input_data, .targets), each shaped [batch_size, num_steps]. The second element    of the tuple is the same data time-shifted to the right by one.\n",
      "\n",
      "  train_input =  Tensor(\"TrainInput/StridedSlice:0\", shape=(20, 35), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "### My implementation ###\n",
    "\"\"\"\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(650)\n",
    "print(lstm_cell.state_size)\n",
    "print(lstm_cell.output_size)\n",
    "initial_state =  tf.zeros([20, 650]),  tf.zeros([20, 650])\n",
    "print(len(initial_state))\n",
    "\"\"\"\n",
    "### Try 2: Zaremba Implementation: Adapted ###\n",
    "\n",
    "#### ____ 1.  Architecture Dimensions ____ ###\n",
    "# You can use the parameters of line below or just use: config=MediumConfig() \n",
    "num_steps=35; hidden_units=650; batch_size=20; layers=2; vocab_size = 10000;\n",
    "\n",
    "class PTBInput(object):\n",
    "  \"\"\"The input data.\"\"\"\n",
    "\n",
    "  def __init__(self, config, data, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    self.input_data, self.targets = reader.ptb_producer(\n",
    "        data, batch_size, num_steps, name=name)\n",
    "    \n",
    "class MediumConfig(object):\n",
    "  \"\"\"Medium config.\"\"\"\n",
    "  init_scale = 0.05\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 650\n",
    "  max_epoch = 6\n",
    "  max_max_epoch = 39\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.8\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "def sample_random_normal(name, mean, std, shape):\n",
    "    \n",
    "    with tf.variable_scope(\"sample_random_normal\"):\n",
    "    \n",
    "        #Inverse softplus (positive std)\n",
    "        standard_dev = tf.log(tf.exp(std) - 1.0) * tf.ones(shape)\n",
    "        \n",
    "        mean = tf.get_variable(name + \"_mean\", initializer=mean, dtype=tf.float32)\n",
    "        standard_deviation = tf.get_variable(name + \"std\", initializer=std, dtype=tf.float32)\n",
    "        #Revert back to std\n",
    "        standard_deviation = tf.nn.softplus(standard_deviation)\n",
    "    \n",
    "        #Sample standard normal\n",
    "        epsilon = tf.random_normal(mean=0, stddev=1, name=\"epsilon\", shape=shape, dtype=tf.float32)\n",
    "      \n",
    "        random_var = mean + standard_deviation*epsilon\n",
    "        print(random_var)\n",
    "    return random_var\n",
    "\n",
    "\"\"\"\n",
    "    Bayesian LSTM Cell framework which inherits from tensorflows BasicLSTMCell\n",
    "    \n",
    "\"\"\"\n",
    "class BayesianLSTMCell(BasicLSTMCell):\n",
    "    def __init__(self,\n",
    "                 mean, #mean for sampling weights\n",
    "                 std, #standard deviation for sampling weights\n",
    "                 config, \n",
    "                 embedding_size,\n",
    "                 # --- below is inherited from BasicLSTMCell --- #\n",
    "                 num_units, #The number of hidden units in the cell (4*num_units)\n",
    "                 forget_bias = 1.0, #bias added to forget gates\n",
    "                 input_size = None, #Deprecated and unused\n",
    "                 state_is_tuple = True, #If True, accepted and returned states are 2-tuples of the c_state and h_state\n",
    "                 activation = tf.tanh):\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.Weights = None\n",
    "        self.Biases = None\n",
    "        \n",
    "        self.config = config\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_steps = config.num_steps\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        #self.max_grad_norm = config.max_grad_norm\n",
    "        #self.learning_rate = config.learning_rate\n",
    "        #self.learning_rate_decay = config.learning_rate_decay\n",
    "        #self.init_scale = config.init_scale\n",
    "        #self.summary_frequency = config.summary_frequency\n",
    "        #self.is_training = is_training\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.forget_bias = forget_bias #NOTE: for some reason they weren't being passed through, so did it explicitly\n",
    "        self.activation = activation\n",
    "        \n",
    "        #From BasicLSTMCell\n",
    "        #super().__init__()\n",
    "        super(BayesianLSTMCell, self).__init__(num_units, forget_bias, input_size, state_is_tuple, activation)\n",
    "    \n",
    "    #Sample weights: dim = [self.embedding_size + self.hidden_size, 4 * self.hidden_size]\n",
    "    def get_weights(self):\n",
    "        with tf.variable_scope(\"CellWeights\"):\n",
    "            self.Weights = sample_random_normal(\"WeightMatrix\", self.mean, self.std, \n",
    "                                                shape = [self.embedding_size + self.hidden_size, 4 * self.hidden_size]) #change shape to variables!\n",
    "            print(\"\\nWeights:\\t\", self.Weights)\n",
    "            return self.Weights\n",
    "    \n",
    "    #Sample biases\n",
    "    def get_biases(self):\n",
    "        with tf.variable_scope(\"CellBiases\"):\n",
    "            self.Biases = sample_random_normal(\"BiasVector\", self.mean, self.std, \n",
    "                                               shape = [4 * self.hidden_size]) #change shape to variable!\n",
    "            print(\"\\nBiases:\\t\", self.Biases)\n",
    "            return self.Biases\n",
    "    \n",
    "    #Object call function\n",
    "    def __call__(self, inputs, state):\n",
    "        with tf.variable_scope(\"BayesLSTMCell\"):  # \"BasicLSTMCell\"\n",
    "            \n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = tf.split(value=state, num_or_size_splits=2, axis=1)\n",
    "            \n",
    "            \n",
    "            all_inputs = tf.concat([inputs, h], 1)\n",
    "            print(\"\\nInputs:\\t\", inputs); print(\"\\nall_inputs:\\t\", all_inputs)\n",
    "            concat = tf.nn.bias_add(tf.matmul(all_inputs, self.get_weights()), self.get_biases()) #self.Biases)\n",
    "            print(\"\\nconcat:\\t\", concat)\n",
    "            \n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = tf.split(value=concat, num_or_size_splits=4, axis=1)\n",
    "            print(\"\\nI Gate:\\t\", i);print(\"\\nJ Gate:\\t\", j);print(\"\\nF Gate:\\t\", f);print(\"\\nO Gate:\\t\", o);\n",
    "            \n",
    "            #Calculate new cell and hidden states. Calculations are as in Zaremba et al 2015\n",
    "            new_c = (c * tf.sigmoid(f + self.forget_bias) + tf.sigmoid(i) * self.activation(j))\n",
    "            new_h = self.activation(new_c) * tf.sigmoid(o)\n",
    "            \n",
    "            \n",
    "            #Create tuple of the new state\n",
    "            new_state = LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "            return new_h, new_state\n",
    "        \n",
    "        \n",
    "#### ____ 2. Loading 1 batch of data ___ ###\n",
    "#input_data is [20, 35] tensor of the data prepared by reader\n",
    "\n",
    "#Current data path\n",
    "path = \"../data/\"\n",
    "\n",
    "### Load data\n",
    "raw_data = reader.ptb_raw_data(path)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "config=MediumConfig()\n",
    "train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "print(\"Train_input contains a pair of Tensors (.input_data, .targets), each shaped [batch_size, num_steps]. The second element\\\n",
    "    of the tuple is the same data time-shifted to the right by one.\\n\\n \",\"train_input = \", train_input.input_data)\n",
    "\n",
    "input_data = train_input.input_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<__main__.BayesianLSTMCell object at 0x11a475080>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "\n",
      "State:\t Tensor(\"BayesianLSTMCellZeroState/zeros:0\", shape=(20, 1300), dtype=float32)\n",
      "\n",
      "Inputs:\t Tensor(\"embedding_lookup:0\", shape=(20, 35, 650), dtype=float32)\n",
      "\n",
      "Inputs:\t Tensor(\"strided_slice:0\", shape=(20, 650), dtype=float32)\n",
      "\n",
      "all_inputs:\t Tensor(\"BayesLSTMCell/concat:0\", shape=(20, 1300), dtype=float32)\n",
      "Tensor(\"BayesLSTMCell/CellWeights/sample_random_normal/add:0\", shape=(1300, 2600), dtype=float32)\n",
      "\n",
      "Weights:\t Tensor(\"BayesLSTMCell/CellWeights/sample_random_normal/add:0\", shape=(1300, 2600), dtype=float32)\n",
      "Tensor(\"BayesLSTMCell/CellBiases/sample_random_normal/add:0\", shape=(2600,), dtype=float32)\n",
      "\n",
      "Biases:\t Tensor(\"BayesLSTMCell/CellBiases/sample_random_normal/add:0\", shape=(2600,), dtype=float32)\n",
      "\n",
      "concat:\t Tensor(\"BayesLSTMCell/BiasAdd:0\", shape=(20, 2600), dtype=float32)\n",
      "\n",
      "I Gate:\t Tensor(\"BayesLSTMCell/split_1:0\", shape=(20, 650), dtype=float32)\n",
      "\n",
      "J Gate:\t Tensor(\"BayesLSTMCell/split_1:1\", shape=(20, 650), dtype=float32)\n",
      "\n",
      "F Gate:\t Tensor(\"BayesLSTMCell/split_1:2\", shape=(20, 650), dtype=float32)\n",
      "\n",
      "O Gate:\t Tensor(\"BayesLSTMCell/split_1:3\", shape=(20, 650), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 3 for 'BayesLSTMCell_1/concat' (op: 'ConcatV2') with input shapes: [20,650], [2,10,650], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 3 for 'BayesLSTMCell_1/concat' (op: 'ConcatV2') with input shapes: [20,650], [2,10,650], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-030d95ab72b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mcell_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3cb9bdfa95e4>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mall_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nInputs:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nall_inputs:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_biases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#self.Biases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1064\u001b[0m   return gen_array_ops._concat_v2(values=values,\n\u001b[1;32m   1065\u001b[0m                                   \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                                   name=name)\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_concat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    491\u001b[0m   \"\"\"\n\u001b[1;32m    492\u001b[0m   result = _op_def_lib.apply_op(\"ConcatV2\", values=values, axis=axis,\n\u001b[0;32m--> 493\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    494\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bryonphillips/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 3 for 'BayesLSTMCell_1/concat' (op: 'ConcatV2') with input shapes: [20,650], [2,10,650], []."
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "#### ____ 3. Setting up LSTM RNN ____ ###\n",
    "#cell = tf.contrib.rnn.BasicLSTMCell(650, forget_bias=0.0, state_is_tuple=True)\n",
    "#cellm = tf.contrib.rnn.MultiRNNCell([cell for _ in range(layers)], state_is_tuple=True)\n",
    "cell = BayesianLSTMCell(mean=0.0, std=1.0, config=config, embedding_size=config.hidden_size, num_units=config.hidden_size)\n",
    "\n",
    "\n",
    "initial_state = cell.zero_state(20, tf.float32) #cellm.zero_state(20, tf.float32)\n",
    "state = initial_state\n",
    "print(\"\\nState:\\t\", state)\n",
    "\n",
    "embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_units], tf.float32)\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data); print(\"\\nInputs:\\t\",inputs); \n",
    "#inputs dimensions: [vocab_size, hidden_units] : this comes from word embeddings\n",
    "\n",
    "#(cell_output, state1) = cell(inputs[:, 0, :], state)\n",
    "#print(\"\\nCell Output:\\t\", cell_output);   print(\"\\nState:\\t\", state1)\n",
    "\n",
    "\n",
    "\n",
    "outputs = []; \n",
    "for time_step in range(num_steps):\n",
    "    (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "    outputs.append(cell_output)\n",
    "output = tf.reshape(tf.concat(outputs, 1), [-1, hidden_units])\n",
    "print(\"\\nOutput:\\t\", output);   print(\"\\nState:\\t\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Implementation 2 Below #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
=======
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
   "outputs": [],
   "source": [
    "#### Functions ####\n",
    "\n",
    "def sample_random_normal(name, mean, std, shape):\n",
    "    \n",
    "    with tf.variable_scope(\"sample_random_normal\"):\n",
    "    \n",
    "        #Inverse softplus (positive std)\n",
    "        standard_dev = tf.log(tf.exp(std) - 1.0) * tf.ones(shape)\n",
    "        \n",
    "        mean = tf.get_variable(name + \"_mean\", initializer=mean, dtype=tf.float32)\n",
    "        standard_deviation = tf.get_variable(name + \"std\", initializer=std, dtype=tf.float32)\n",
    "        #Revert back to std\n",
    "        standard_deviation = tf.nn.softplus(standard_deviation)\n",
    "    \n",
    "        #Sample standard normal\n",
    "        epsilon = tf.random_normal(mean=0, stddev=1, name=\"epsilon\", shape=shape, dtype=tf.float32)\n",
    "      \n",
    "        random_var = mean + standard_deviation*epsilon\n",
    "    \n",
    "    return random_var\n",
    "\n",
    "\n",
    "#### Borrowed from PTB model ####\n",
    "\n",
    "class PTBInput(object):\n",
    "  \"\"\"The input data.\"\"\"\n",
    "\n",
    "  def __init__(self, config, data, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    #self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    self.input_data, self.targets = reader.ptb_producer(\n",
    "        data, batch_size, num_steps, name=name)\n",
    "    \n",
    "\n",
    "    \n",
    "class TestConfig(object):\n",
    "  \"\"\"Tiny config, for testing.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 1\n",
    "  num_layers = 1\n",
    "  num_steps = 2\n",
    "  hidden_size = 2\n",
    "  max_epoch = 1\n",
    "  max_max_epoch = 1\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "  #rnn_mode = BLOCK\n",
    "\n",
    "class MediumConfig(object):\n",
    "  \"\"\"Medium config.\"\"\"\n",
    "  init_scale = 0.05\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 650\n",
    "  max_epoch = 6\n",
    "  max_max_epoch = 39\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.8\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "  #rnn_mode = BLOCK\n",
    "\n",
    "#### End of borrowed from PTB model ####\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "    Bayesian LSTM Cell framework which inherits from tensorflows BasicLSTMCell\n",
    "    \n",
    "\"\"\"\n",
    "class BayesianLSTMCell(BasicLSTMCell):\n",
    "    def __init__(self,\n",
    "                 mean, #mean for sampling weights\n",
    "                 std, #standard deviation for sampling weights\n",
    "                 # --- below is inherited from BasicLSTMCell --- #\n",
    "                 num_units, #The number of hidden units in the cell (4*num_units)\n",
    "                 forget_bias = 1.0, #bias added to forget gates\n",
    "                 input_size = None, #Deprecated and unused\n",
    "                 state_is_tuple = True, #If True, accepted and returned states are 2-tuples of the c_state and h_state\n",
    "                 activation = tf.tanh):\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.Weights = None\n",
    "        self.Biases = None\n",
    "        \n",
    "        #From BasicLSTMCell\n",
    "        #super().__init__()\n",
    "        super(BayesianLSTMCell, self).__init__(num_units, forget_bias, input_size, state_is_tuple, activation)\n",
    "    \n",
    "    #Sample weights\n",
    "    def get_weights(self):\n",
    "        with tf.variable_scope(\"CellWeights\"):\n",
    "            self.Weights = sample_random_normal(\"WeightMatrix\", self.mean, self.std, shape = [2*self.num_units,4*self.num_units]) #change shape to variables!\n",
    "            return self.Weights\n",
    "    \n",
    "    #Sample biases\n",
    "    def get_biases(self):\n",
    "        with tf.variable_scope(\"CellBiases\"):\n",
    "            self.Biases = sample_random_normal(\"BiasVector\", self.mean, self.std, shape = [4*self.num_units]) #change shape to variable!\n",
    "            return self.Biases\n",
    "    \n",
    "    #Object call function\n",
    "    def __call__(self, inputs, state):\n",
    "        with tf.variable_scope(\"BayesLSTMCell\"):  # \"BasicLSTMCell\"\n",
    "            \n",
    "            #State is a tuple with the current cell and hidden state vectors\n",
    "            cell, hidden = state\n",
    "            \n",
    "            all_inputs = tf.concat([inputs, hidden], 1)\n",
    "            \n",
    "            Weights = self.get_weights()\n",
    "            Biases = self.get_biases()\n",
    "            \n",
    "            # Wx + b\n",
    "            # For hidden_units = 2, W should be a 8x4 matrix and bias = 8 long vector.\n",
    "            # since previous hidden state h = 2 and current input = 2\n",
    "            #\n",
    "            concat =  tf.nn.bias_add(tf.matmul(all_inputs, Weights), Biases)\n",
    "\n",
    "            #Split data up for the 4 gates\n",
    "            #\n",
    "            #Justify the splitting below. What is actually happening?\n",
    "            #\n",
    "            #i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = tf.split(concat, axis = 1, num_or_size_splits = 4)\n",
    "\n",
    "            #Calculate new cell and hidden states. Calculations are as in Zaremba et al 2015\n",
    "            new_cell = (cell * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i)*self._activation(j))\n",
    "            new_hidden = self._activation(new_cell) * tf.sigmoid(o)\n",
    "            \n",
    "            #Create tuple of the new state\n",
    "            new_state = LSTMStateTuple(new_cell, new_hidden)\n",
    "\n",
    "            return new_hidden, new_state"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x1f4074ad320>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
   "source": [
    "#Initiate Tensorboard\n",
    "\n",
    "subprocess.Popen([\"tensorboard\",\"--logdir=tensorboard\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 36,
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Process data ####\n",
    "##\n",
    "## Currently we're only interested in running a minimal sample\n",
    "## To verify graphs/sessions and such\n",
    "##\n",
    "####\n",
    "tf.reset_default_graph()\n",
    "\n",
    "### Set initial parameters\n",
    "\n",
    "#Current data path\n",
    "path = \"../data/\"\n",
    "\n",
    "### Load data\n",
    "raw_data = reader.ptb_raw_data(path)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
<<<<<<< HEAD
    "config=TestConfig()\n",
    "train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "\n",
=======
    "config = TestConfig()\n",
    "\n",
    "train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
    "\n",
    "#collect input data and targets by calling the ptb_producer function\n",
    "#input_data, targets = reader.ptb_producer(train_data, batch_size, num_steps, name = \"TrainInput\")\n",
    "\n",
    "#Embed the data: Embedding format is [vocab_size, hidden_size] = [20, 2]\n",
    "embedding = tf.get_variable(\"embedding\", [config.vocab_size, config.hidden_size], dtype=tf.float32)\n",
    "#inputs = [20, 1, 2] tensor containing 20 words for one timestep with 2 hidden units\n",
    "input = tf.nn.embedding_lookup(embedding, train_input.input_data)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
=======
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<__main__.BayesianLSTMCell object at 0x000001F40456B2E8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'CellWeights/sample_random_normal/add:0' shape=(4, 8) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
   "source": [
    "#### Build the graph ####\n",
    "\n",
    "#Just setting some values for testing implementation\n",
    "\n",
<<<<<<< HEAD
    "# Args: BayesianLSTMCell(mean, std, inputs, state)\n",
    "#    inputs: 2-D tensor with shape [batch_size x input_size].\n",
    "#    state: if self.state_size is an integer, this should be a 2-D Tensor with shape [batch_size x self.state_size]. Otherwise, if self.state_size is a tuple of integers, this should be a tuple with shapes [batch_size x s] for s in self.state_size.\n",
    "\n",
    "cell = BayesianLSTMCell(mean=0.0, std=1.0, num_units=config.hidden_size)\n",
    "#cell = MultiRNNCell([cell]*2, state_is_tuple=True)\n",
    "#cell = (BasicLSTMCell(num_units=hidden_size))\n",
    "print(cell.state_size)\n",
    "print(cell.output_size)\n",
    "#c = cell.zero_state(20, tf.float32)\n",
    "#h = cell.zero_state(20, tf.float32)\n",
    "#state = (c,h)\n",
    "\n",
    "#cell(inputs=input[:,0,:], state=state)\n",
    "\"\"\"\"\n",
    "outputs = []; \n",
    "for time_step in range(num_steps):\n",
    "    (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "    outputs.append(cell_output)\n",
    "output = tf.reshape(tf.concat(outputs, 1), [-1, hidden_units])\n",
    "print(\"\\nOutput:\\t\", output);   print(\"\\nState:\\t\", state)\n",
    "\"\"\""
=======
    "cell = BayesianLSTMCell(mean=0.0, std=1.0, num_units=config.hidden_size, state_is_tuple=True)\n",
    "#cell = MultiRNNCell([cell]*2, state_is_tuple=True)\n",
    "#cell = (BasicLSTMCell(num_units=config.hidden_size))\n",
    "\n",
    "#print(cell.state_size)\n",
    "#c = cell.zero_state(config.batch_size, tf.float32)\n",
    "#h = cell.zero_state(config.batch_size, tf.float32)\n",
    "#state = (c,h)\n",
    "#print(state)\n",
    "\n",
    "#print(input[:,0,:])\n",
    "\n",
    "#state = cell.zero_state(batch_size=config.batch_size, dtype=tf.float32)\n",
    "\n",
    "#cell(inputs=input[:,0,:], state=state)"
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
=======
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 3679d4010c8754f5ea7856ad651b7c2fe8946e7f
   "outputs": [],
   "source": [
    "#Run a session on the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #Create event file for tensorboard\n",
    "    summary_write = tf.summary.FileWriter((\"tensorboard\"),sess.graph)\n",
    "    \n",
    "    #Feed data to a placeholder\n",
    "    #feed_dict = {}\n",
    "    #Run desired stuff\n",
    "    #Y_out = sess.run(Y, feed_dict = feed_dict)\n",
    "    \n",
    "#print(Y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
