{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Import ####\n",
    "import subprocess\n",
    "import reader\n",
    "import tensorflow as tf\n",
    "\n",
    "#For Bayesian LSTM cell implementation\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple, MultiRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions ####\n",
    "\n",
    "def sample_random_normal(name, mean, std, shape):\n",
    "    \n",
    "    with tf.variable_scope(\"sample_random_normal\"):\n",
    "    \n",
    "        #Inverse softplus (positive std)\n",
    "        standard_dev = tf.log(tf.exp(std) - 1.0) * tf.ones(shape)\n",
    "        \n",
    "        mean = tf.get_variable(name + \"_mean\", initializer=mean, dtype=tf.float32)\n",
    "        standard_deviation = tf.get_variable(name + \"std\", initializer=std, dtype=tf.float32)\n",
    "        #Revert back to std\n",
    "        standard_deviation = tf.nn.softplus(standard_deviation)\n",
    "    \n",
    "        #Sample standard normal\n",
    "        epsilon = tf.random_normal(mean=0, stddev=1, name=\"epsilon\", shape=shape, dtype=tf.float32)\n",
    "      \n",
    "        random_var = mean + standard_deviation*epsilon\n",
    "    \n",
    "    return random_var\n",
    "\n",
    "\n",
    "#### Borrowed from PTB model ####\n",
    "\n",
    "class PTBInput(object):\n",
    "  \"\"\"The input data.\"\"\"\n",
    "\n",
    "  def __init__(self, config, data, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    #self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    self.input_data, self.targets = reader.ptb_producer(\n",
    "        data, batch_size, num_steps, name=name)\n",
    "    \n",
    "\n",
    "    \n",
    "class TestConfig(object):\n",
    "  \"\"\"Tiny config, for testing.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 1\n",
    "  num_layers = 2\n",
    "  num_steps = 5\n",
    "  hidden_size = 20\n",
    "  max_epoch = 1\n",
    "  max_max_epoch = 1\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "  #rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "#### End of borrowed from PTB model ####\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "    Bayesian LSTM Cell framework which inherits from tensorflows BasicLSTMCell\n",
    "    \n",
    "        Input parameters:\n",
    "            On initialization:\n",
    "                -mean = the mean to be used when sampling\n",
    "                -std = the standard deviation to be used when sampling\n",
    "                -num_units = the number of hidden units in each gate\n",
    "                -**kwargs is the remaining arguments inherited from BasicLSTMCell\n",
    "            \n",
    "        On call:\n",
    "                -inputs = input data i.e. embedded words\n",
    "                -state = A state tuple (c,h) containing the cell and hidden state\n",
    "                from the previous LSTM cell\n",
    "        \n",
    "        Output parameters:\n",
    "            -On initialization: \n",
    "                Creates a BayesianLSTMCell class with inherited properties from BasicLSTMCell\n",
    "                and the functions given in the class\n",
    "            \n",
    "            -On call:\n",
    "                Outputs the current state tuple (c,h) containing the current cell and hidden\n",
    "                state of the LSTMCell\n",
    "    \n",
    "\"\"\"\n",
    "class BayesianLSTMCell(BasicLSTMCell):\n",
    "    def __init__(self, mean, std, num_units, **kwargs):\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.num_units = num_units\n",
    "        self.Weights = None\n",
    "        self.Biases = None\n",
    "        \n",
    "        #From BasicLSTMCell\n",
    "        super(BayesianLSTMCell, self).__init__(num_units, **kwargs)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        A note on the shape for the sampling of weights and biases:\n",
    "        \n",
    "        The number of weights we want to sample is determined by the hidden state\n",
    "        from the previous cell, the input data, and of course the number of gates\n",
    "        in a single cell.\n",
    "        \n",
    "        Assume that num_units = 10:\n",
    "        There's 4 gates in an LSTM cell. Hence when we say we want num_units = 10\n",
    "        then the LSTM cell actually consists of 4*num_units = 40 hidden units.\n",
    "        \n",
    "        The hidden state from the previous gate will have the shape h=[num_units]=[10]\n",
    "        The input data is embedded and in the original model embedding_size = num_units\n",
    "        This can of course be changed if wanted. So input data will have the shape\n",
    "        inputs=[embedding_size]=[num_units]=[10]\n",
    "        \n",
    "        Input data and the hidden state from the previous cell is concatenated before passed\n",
    "        to any gate. Thus the input to each gate will be x = [embedding_size + num_units] = [20]\n",
    "        that is, a 20 long vector.\n",
    "        \n",
    "        So the total amount of weights needed will be 4*num_units*(embedding_size+num_units) = 160\n",
    "        The total amount of biases is just the length of the input vector x to the gates, so the\n",
    "        total number of biases should be embedding_size + num_units = 20\n",
    "    \"\"\"\n",
    "    \n",
    "    #Sample weights\n",
    "    def get_weights(self):\n",
    "        with tf.variable_scope(\"CellWeights\"):\n",
    "            self.Weights = sample_random_normal(\"WeightMatrix\",\n",
    "                                                self.mean,\n",
    "                                                self.std,\n",
    "                                                shape = [2*self.num_units,4*self.num_units])\n",
    "            return self.Weights\n",
    "    \n",
    "    #Sample biases\n",
    "    def get_biases(self):\n",
    "        with tf.variable_scope(\"CellBiases\"):\n",
    "            self.Biases = sample_random_normal(\"BiasVector\",\n",
    "                                               self.mean,\n",
    "                                               self.std,\n",
    "                                               shape = [4*self.num_units])\n",
    "            return self.Biases\n",
    "    \n",
    "    #Class call function\n",
    "    def __call__(self, inputs, state):\n",
    "        with tf.variable_scope(\"BayesLSTMCell\"):\n",
    "            \n",
    "            #State is a tuple with the cell and hidden state vectors from\n",
    "            #the previous BayesianLSTMCell\n",
    "            cell, hidden = state\n",
    "            \n",
    "            #Vector concatenation of previous hidden state and embedded inputs\n",
    "            concat_inputs_hidden = tf.concat([inputs, hidden], 1)\n",
    "            \n",
    "            #Sample weights and biases\n",
    "            Weights = self.get_weights()\n",
    "            Biases = self.get_biases()\n",
    "            \n",
    "            \"\"\"\n",
    "                gate_inputs is basically the calculation Wx + b of ALL gates.\n",
    "                Take e.g. num_units = 2. Thus total number of hidden_units = 8.\n",
    "                The input vector x in this case is a 2 long vector, as is the hidden\n",
    "                state vector. So dimensions are W = 4x8, x = 4 and b = 8.\n",
    "                Then we can do Wx + b and get an 8 long vector which can be passed\n",
    "                through the 4 gates and their respective activation functions.\n",
    "            \"\"\"\n",
    "            with tf.variable_scope(\"gateOps\"):\n",
    "            \n",
    "                #gate_inputs =  tf.nn.bias_add(tf.matmul(concat_inputs_hidden, Weights), Biases)\n",
    "                gate_inputs =  tf.nn.xw_plus_b(x=concat_inputs_hidden, weights=Weights, biases=Biases)\n",
    "\n",
    "                #Split data up for the 4 gates\n",
    "                #i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "                i, j, f, o = tf.split(gate_inputs, axis = 1, num_or_size_splits = 4)\n",
    "\n",
    "                \"\"\"\n",
    "                    Calculate new cell and new hidden states. Calculations are as in Zaremba et al 2015:\n",
    "                \n",
    "                    new_cell = cell*\\sigma(f + bias) + \\sigma(i)*\\sigma(j)\n",
    "                    new_hidden = \\tanh(new_cell)*\\sigma(o)\n",
    "                \n",
    "                    See the LSTM graph here: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "                \n",
    "                \"\"\"\n",
    "                new_cell = (cell * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i)*self._activation(j))\n",
    "                new_hidden = self._activation(new_cell) * tf.sigmoid(o)\n",
    "            \n",
    "            #Create tuple of the new state\n",
    "            new_state = LSTMStateTuple(new_cell, new_hidden)\n",
    "\n",
    "            return new_hidden, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x2131d0343c8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiate Tensorboard\n",
    "\n",
    "subprocess.Popen([\"tensorboard\",\"--logdir=tensorboard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Process data ####\n",
    "##\n",
    "## Currently we're only interested in running a minimal sample\n",
    "## To verify graphs/sessions and such\n",
    "##\n",
    "####\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "### Set initial parameters\n",
    "\n",
    "#Current data path\n",
    "path = \"../data/\"\n",
    "\n",
    "### Load data\n",
    "raw_data = reader.ptb_raw_data(path)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "config = TestConfig()\n",
    "\n",
    "train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "\n",
    "#collect input data and targets by calling the ptb_producer function\n",
    "#input_data, targets = reader.ptb_producer(train_data, batch_size, num_steps, name = \"TrainInput\")\n",
    "\n",
    "#Embed the data: Embedding format is [vocab_size, hidden_size] = [20, 2]\n",
    "embedding = tf.get_variable(\"embedding\", [config.vocab_size, config.hidden_size], dtype=tf.float32)\n",
    "#inputs = [20, 1, 2] tensor containing 20 words for one timestep with 2 hidden units\n",
    "input = tf.nn.embedding_lookup(embedding, train_input.input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Build the graph ####\n",
    "\n",
    "\n",
    "\n",
    "#Initialize a single instance of a Bayesian LSTM cell\n",
    "cell = BayesianLSTMCell(mean=0.0, std=1.0, num_units=config.hidden_size, state_is_tuple=True)\n",
    "#Create a 2 layer BRNN using the MultiRNNCell wrapper\n",
    "cell = MultiRNNCell([cell for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "#Set initial state of the BRNN to zeros (i.e. at time zero we simply initialize the state to zero.)\n",
    "with tf.variable_scope(\"InitialState\"):\n",
    "    state = cell.zero_state(batch_size=20, dtype=tf.float32)\n",
    "\n",
    "#Unroll the BRNN for as many time steps as defined in config.num_steps\n",
    "outputs = []\n",
    "with tf.variable_scope(\"BRNN\"):\n",
    "    for time_step in range(config.num_steps):\n",
    "        #We reuse the variable name for all unrolls\n",
    "        if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "        #Getting the output and the state for each time step\n",
    "        (cell_output, state) = cell(inputs=input[:,time_step,:], state=state)\n",
    "        #Collect outputs from cells in outputs array\n",
    "        outputs.append(cell_output)\n",
    "    #Reshaping. Not sure why\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
    "\n",
    "\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [config.hidden_size, config.vocab_size], dtype=tf.float32)\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [config.vocab_size], dtype=tf.float32)\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "# Reshape logits to be a 3-D tensor for sequence loss\n",
    "logits = tf.reshape(logits, [config.batch_size, config.num_steps, config.vocab_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run a session on the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #Create event file for tensorboard\n",
    "    summary_write = tf.summary.FileWriter((\"tensorboard\"),sess.graph)\n",
    "    \n",
    "    #Feed data to a placeholder\n",
    "    #feed_dict = {}\n",
    "    #Run desired stuff\n",
    "    #Y_out = sess.run(Y, feed_dict = feed_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
