{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brnn_model_fake_data import *\n",
    "import reader\n",
    "\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import import_folders\n",
    "import pickle_lib as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Global variables\n",
    "\"\"\"\n",
    "model_type = \"test\"\n",
    "data_path = \"../data/\"\n",
    "save_path = \"./saved_model/\"\n",
    "global_prior_pi = 0.25\n",
    "global_log_sigma1 = -1.0\n",
    "global_log_sigma2 = -7.0\n",
    "global_random_seed = 12\n",
    "global_num_gpus = 0\n",
    "\n",
    "\n",
    "# Model can be \"test\", \"small\", \"medium\", \"large\"\n",
    "model_select = \"test\"\n",
    "model_type = model_select\n",
    "#Put the path to the data here\n",
    "dat_path = \"../data\"\n",
    "\n",
    "#Put the path to where you want to save the training data\n",
    "sav_path = \"tensorboard/\"\n",
    "\n",
    "# The mixing degree for the prior gaussian mixture\n",
    "# As in Fortunato they report scanning\n",
    "# mix_pi \\in { 1/4, 1/2, 3/4 }\n",
    "mixing_pi = 0.25\n",
    "\n",
    "# As in Fortunato they report scanning\n",
    "# log sigma1 \\in { 0, -1, -2 }\n",
    "# log sigma2 \\in { -6, -7, -8 }\n",
    "prior_log_sigma1 = -1.0\n",
    "prior_log_sigma2 = -7.0\n",
    "\n",
    "\n",
    "class SmallConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    \n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    \n",
    "    X_dim = 200 # Size of the embedding\n",
    "\n",
    "class MediumConfig(object):\n",
    "    \"\"\"\n",
    "    Medium config.\n",
    "    Slightly modified according to email.\n",
    "    \"\"\"\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 20\n",
    "    max_max_epoch = 70\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.9\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "    X_dim = 50 # Size of the embedding\n",
    "    \n",
    "class LargeConfig(object):\n",
    "    \"\"\"Large config.\"\"\"\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1 / 1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "    X_dim = 100 # Size of the embedding\n",
    "    \n",
    "class TestConfig(object):\n",
    "    \"\"\"Tiny config, for testing.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 15\n",
    "    max_epoch = 10\n",
    "    max_max_epoch = 10\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 13\n",
    "    \n",
    "    vocab_size = 10000\n",
    "\n",
    "    X_dim = 19 # Size of the embedding\n",
    "\n",
    "\n",
    "#    global_random_seed = set_random_seed\n",
    "    \n",
    "def get_config():\n",
    "    \"\"\"Get model config.\"\"\"\n",
    "    if model_type == \"small\":\n",
    "        config = SmallConfig()\n",
    "    elif model_type == \"medium\":\n",
    "        config = MediumConfig()\n",
    "    elif model_type == \"large\":\n",
    "        config = LargeConfig()\n",
    "    elif model_type == \"test\":\n",
    "        config = TestConfig()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model: %s\", model_type)\n",
    "\n",
    "    print (\"Model Type\")\n",
    "    print (model_type)\n",
    "    config.prior_pi = global_prior_pi\n",
    "    config.log_sigma1 = global_log_sigma1\n",
    "    config.log_sigma2 = global_log_sigma2\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Model Type\n",
      "test\n",
      "Model Type\n",
      "test\n",
      "Number of total initial chains 20000\n",
      "Dimensionality of chains (num_step,X_dim) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (model_type)\n",
    "\n",
    "####### Global data reading #########\n",
    "Ndivisions = 10;\n",
    "folder_data = \"./data/artificial/\"\n",
    "\n",
    "X_list = pkl.load_pickle(folder_data +\"X_values.pkl\",Ndivisions)\n",
    "Y_list = pkl.load_pickle(folder_data +\"Y_values.pkl\",Ndivisions)\n",
    "\n",
    "num_steps, X_dim = X_list[0].shape\n",
    "num_chains = len(X_list)\n",
    "\n",
    "\n",
    "## Divide in train val and test\n",
    "proportion_tr = 0.8\n",
    "proportion_val = 0.1\n",
    "proportion_tst = 1 -( proportion_val + proportion_tr)\n",
    "\n",
    "num_tr = 10000\n",
    "num_val = 5000\n",
    "num_tst = 5000\n",
    "\n",
    "train_X = [X_list[i] for i in range(num_tr)]\n",
    "train_Y = [Y_list[i] for i in range(num_tr)]\n",
    "\n",
    "val_X = [X_list[i] for i in range(num_tr, num_tr + num_val)]\n",
    "val_Y = [Y_list[i] for i in range(num_tr, num_tr + num_val)]\n",
    "\n",
    "tst_X = [X_list[i] for i in range(num_tr + num_val,  num_tr + num_val + num_tst)]\n",
    "tst_Y = [Y_list[i] for i in range(num_tr + num_val,  num_tr + num_val + num_tst)]\n",
    "\n",
    "# Create the objects with the hyperparameters that will be fed to the network\n",
    "train_config = get_config()\n",
    "eval_config = get_config( )\n",
    "\n",
    "###### Over Set parameters #####\n",
    "train_config.X_dim  = X_dim\n",
    "eval_config.X_dim  = X_dim\n",
    "train_config.num_steps  = num_steps\n",
    "eval_config.num_steps  = num_steps\n",
    "\n",
    "train_config.vocab_size = 2\n",
    "eval_config.vocab_size= 2\n",
    "\n",
    "\n",
    "eval_config.batch_size = 2\n",
    "    \n",
    "#eval_config.num_steps = 1\n",
    "\n",
    "print (\"Number of total initial chains %i\"%len(X_list))\n",
    "print (\"Dimensionality of chains (num_step,X_dim)\",X_list[0].shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name KL Loss is illegal; using KL_Loss instead.\n",
      "INFO:tensorflow:Summary name Total Loss is illegal; using Total_Loss instead.\n",
      "Creating Validation model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#subprocess.Popen([\"tensorboard\",\"--logdir=tensorboard\"])\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-train_config.init_scale,\n",
    "                                                train_config.init_scale)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = BBB_LSTM_Artificial_Data_Input(batch_size = train_config.batch_size, \n",
    "                                                        X = train_X, Y = train_Y,  name=\"TrainInput\")\n",
    "        \n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True, config=train_config, input_=train_input)\n",
    "        tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "        tf.summary.scalar(\"KL Loss\", m.kl_loss)\n",
    "        tf.summary.scalar(\"Total Loss\", m.total_loss)\n",
    "\n",
    "    print (\"Creating Validation model\")\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = BBB_LSTM_Artificial_Data_Input(batch_size = eval_config.batch_size, \n",
    "                                                            X = val_X, Y = val_Y,  name=\"ValidInput\")\n",
    "        \n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = PTBModel(is_training=False, config=eval_config, input_=valid_input)\n",
    "        tf.summary.scalar(\"Validation_Loss\", mvalid.cost)\n",
    "\n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = BBB_LSTM_Artificial_Data_Input(batch_size = eval_config.batch_size, \n",
    "                                                            X = tst_X, Y = tst_Y,  name=\"TestInput\")\n",
    "            \n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False, config=eval_config,\n",
    "                             input_=test_input)\n",
    "\n",
    "    models = {\"Train\": m, \"Valid\": mvalid, \"Test\": mtest}\n",
    "    for name, model in models.items():\n",
    "        model.export_ops(name)\n",
    "    metagraph = tf.train.export_meta_graph()\n",
    "    soft_placement = False\n",
    "    if global_num_gpus > 1:\n",
    "        soft_placement = True\n",
    "        util.auto_parallel(metagraph, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-7690\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./saved_model/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 7690.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "0.000 perplexity: 1.287 speed: 556 wps\n",
      "KL is 1.0881880521774292\n",
      "0.013 perplexity: 1.215 speed: 4632 wps\n",
      "KL is 1.0752440690994263\n",
      "0.112 perplexity: 1.234 speed: 20499 wps\n",
      "KL is 1.0817126035690308\n",
      "0.211 perplexity: 1.230 speed: 27543 wps\n",
      "KL is 1.080313801765442\n",
      "0.309 perplexity: 1.234 speed: 31824 wps\n",
      "KL is 1.0907245874404907\n",
      "0.408 perplexity: 1.234 speed: 36544 wps\n",
      "KL is 1.1031640768051147\n",
      "0.507 perplexity: 1.234 speed: 37877 wps\n",
      "KL is 1.0963314771652222\n",
      "0.606 perplexity: 1.233 speed: 40915 wps\n",
      "KL is 1.0994136333465576\n",
      "0.705 perplexity: 1.233 speed: 43415 wps\n",
      "KL is 1.0911576747894287\n",
      "0.804 perplexity: 1.232 speed: 45517 wps\n",
      "KL is 1.092376708984375\n",
      "0.902 perplexity: 1.233 speed: 47389 wps\n",
      "KL is 1.1040195226669312\n",
      "Epoch: 1 Train Perplexity: 1.232\n",
      "Epoch: 1 Valid Perplexity: 1.359\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "0.000 perplexity: 1.286 speed: 55807 wps\n",
      "KL is 1.1004323959350586\n",
      "0.013 perplexity: 1.228 speed: 71059 wps\n",
      "KL is 1.0983806848526\n",
      "0.112 perplexity: 1.231 speed: 70763 wps\n",
      "KL is 1.1020159721374512\n",
      "0.211 perplexity: 1.229 speed: 70185 wps\n",
      "KL is 1.101699709892273\n",
      "0.309 perplexity: 1.230 speed: 70613 wps\n",
      "KL is 1.110498070716858\n",
      "0.408 perplexity: 1.230 speed: 70738 wps\n",
      "KL is 1.1143532991409302\n",
      "0.507 perplexity: 1.230 speed: 70436 wps\n",
      "KL is 1.1076868772506714\n",
      "0.606 perplexity: 1.229 speed: 70328 wps\n",
      "KL is 1.1131147146224976\n",
      "0.705 perplexity: 1.229 speed: 70385 wps\n",
      "KL is 1.1089507341384888\n",
      "0.804 perplexity: 1.228 speed: 68907 wps\n",
      "KL is 1.1064729690551758\n",
      "0.902 perplexity: 1.228 speed: 68779 wps\n",
      "KL is 1.1139477491378784\n",
      "Epoch: 2 Train Perplexity: 1.228\n",
      "Epoch: 2 Valid Perplexity: 1.234\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "0.000 perplexity: 1.289 speed: 47258 wps\n",
      "KL is 1.1187307834625244\n",
      "0.013 perplexity: 1.227 speed: 66357 wps\n",
      "KL is 1.105454683303833\n",
      "0.112 perplexity: 1.227 speed: 72272 wps\n",
      "KL is 1.1187281608581543\n",
      "0.211 perplexity: 1.226 speed: 57184 wps\n",
      "KL is 1.1150137186050415\n",
      "0.309 perplexity: 1.227 speed: 54848 wps\n",
      "KL is 1.1055840253829956\n",
      "0.408 perplexity: 1.227 speed: 54105 wps\n",
      "KL is 1.1165636777877808\n",
      "0.507 perplexity: 1.228 speed: 53027 wps\n",
      "KL is 1.1265090703964233\n",
      "0.606 perplexity: 1.228 speed: 55265 wps\n",
      "KL is 1.1281499862670898\n",
      "0.705 perplexity: 1.227 speed: 57005 wps\n",
      "KL is 1.1262261867523193\n",
      "0.804 perplexity: 1.225 speed: 58255 wps\n",
      "KL is 1.1334401369094849\n",
      "0.902 perplexity: 1.225 speed: 59353 wps\n",
      "KL is 1.1254103183746338\n",
      "Epoch: 3 Train Perplexity: 1.224\n",
      "Epoch: 3 Valid Perplexity: 1.216\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "0.000 perplexity: 1.234 speed: 52946 wps\n",
      "KL is 1.1256864070892334\n",
      "0.013 perplexity: 1.232 speed: 65222 wps\n",
      "KL is 1.1219168901443481\n",
      "0.112 perplexity: 1.221 speed: 70083 wps\n",
      "KL is 1.1311744451522827\n",
      "0.211 perplexity: 1.220 speed: 69925 wps\n",
      "KL is 1.123694896697998\n",
      "0.309 perplexity: 1.221 speed: 69646 wps\n",
      "KL is 1.1287211179733276\n",
      "0.408 perplexity: 1.222 speed: 65084 wps\n",
      "KL is 1.1325342655181885\n",
      "0.507 perplexity: 1.224 speed: 60138 wps\n",
      "KL is 1.1309539079666138\n",
      "0.606 perplexity: 1.223 speed: 60500 wps\n",
      "KL is 1.1356444358825684\n",
      "0.705 perplexity: 1.224 speed: 61286 wps\n",
      "KL is 1.132898211479187\n",
      "0.804 perplexity: 1.224 speed: 62023 wps\n",
      "KL is 1.1417815685272217\n",
      "0.902 perplexity: 1.223 speed: 62829 wps\n",
      "KL is 1.1410428285598755\n",
      "Epoch: 4 Train Perplexity: 1.224\n",
      "Epoch: 4 Valid Perplexity: 1.195\n",
      "Epoch: 5 Learning rate: 1.000\n",
      "0.000 perplexity: 1.218 speed: 57757 wps\n",
      "KL is 1.1446740627288818\n",
      "0.013 perplexity: 1.203 speed: 34080 wps\n",
      "KL is 1.145445466041565\n",
      "0.112 perplexity: 1.219 speed: 55187 wps\n",
      "KL is 1.1507816314697266\n",
      "0.211 perplexity: 1.217 speed: 61660 wps\n",
      "KL is 1.1486927270889282\n",
      "0.309 perplexity: 1.220 speed: 64602 wps\n",
      "KL is 1.1402113437652588\n",
      "0.408 perplexity: 1.221 speed: 66361 wps\n",
      "KL is 1.1473625898361206\n",
      "0.507 perplexity: 1.222 speed: 66947 wps\n",
      "KL is 1.1570698022842407\n",
      "0.606 perplexity: 1.222 speed: 67550 wps\n",
      "KL is 1.1515742540359497\n",
      "0.705 perplexity: 1.221 speed: 68072 wps\n",
      "KL is 1.1531251668930054\n",
      "0.804 perplexity: 1.220 speed: 68364 wps\n",
      "KL is 1.1531729698181152\n",
      "0.902 perplexity: 1.220 speed: 68474 wps\n",
      "KL is 1.1530555486679077\n",
      "Epoch: 5 Train Perplexity: 1.220\n",
      "Epoch: 5 Valid Perplexity: 1.216\n",
      "Epoch: 6 Learning rate: 1.000\n",
      "0.000 perplexity: 1.168 speed: 47135 wps\n",
      "KL is 1.1632243394851685\n",
      "0.013 perplexity: 1.206 speed: 67888 wps\n",
      "KL is 1.1520613431930542\n",
      "0.112 perplexity: 1.212 speed: 70497 wps\n",
      "KL is 1.1522185802459717\n",
      "0.211 perplexity: 1.210 speed: 70901 wps\n",
      "KL is 1.1480871438980103\n",
      "0.309 perplexity: 1.214 speed: 71042 wps\n",
      "KL is 1.1509791612625122\n",
      "0.408 perplexity: 1.215 speed: 70887 wps\n",
      "KL is 1.1583929061889648\n",
      "0.507 perplexity: 1.215 speed: 71218 wps\n",
      "KL is 1.1670045852661133\n",
      "0.606 perplexity: 1.216 speed: 70992 wps\n",
      "KL is 1.1583447456359863\n",
      "0.705 perplexity: 1.216 speed: 70943 wps\n",
      "KL is 1.1585135459899902\n",
      "0.804 perplexity: 1.215 speed: 70975 wps\n",
      "KL is 1.1518735885620117\n",
      "0.902 perplexity: 1.215 speed: 70974 wps\n",
      "KL is 1.1619759798049927\n",
      "Epoch: 6 Train Perplexity: 1.215\n",
      "Epoch: 6 Valid Perplexity: 1.184\n",
      "Epoch: 7 Learning rate: 1.000\n",
      "0.000 perplexity: 1.171 speed: 45442 wps\n",
      "KL is 1.161362648010254\n",
      "0.013 perplexity: 1.207 speed: 69009 wps\n",
      "KL is 1.1657860279083252\n",
      "0.112 perplexity: 1.211 speed: 51655 wps\n",
      "KL is 1.158066749572754\n",
      "0.211 perplexity: 1.208 speed: 50654 wps\n",
      "KL is 1.161405086517334\n",
      "0.309 perplexity: 1.211 speed: 55700 wps\n",
      "KL is 1.17062509059906\n",
      "0.408 perplexity: 1.213 speed: 58770 wps\n",
      "KL is 1.1643693447113037\n",
      "0.507 perplexity: 1.213 speed: 60847 wps\n",
      "KL is 1.16194748878479\n",
      "0.606 perplexity: 1.214 speed: 59867 wps\n",
      "KL is 1.168455719947815\n",
      "0.705 perplexity: 1.214 speed: 61245 wps\n",
      "KL is 1.1635937690734863\n",
      "0.804 perplexity: 1.213 speed: 59332 wps\n",
      "KL is 1.1679723262786865\n",
      "0.902 perplexity: 1.214 speed: 58200 wps\n",
      "KL is 1.1699799299240112\n",
      "Epoch: 7 Train Perplexity: 1.213\n",
      "Epoch: 7 Valid Perplexity: 1.207\n",
      "Epoch: 8 Learning rate: 1.000\n",
      "0.000 perplexity: 1.185 speed: 55735 wps\n",
      "KL is 1.1613614559173584\n",
      "0.013 perplexity: 1.210 speed: 66084 wps\n",
      "KL is 1.1702065467834473\n",
      "0.112 perplexity: 1.213 speed: 68647 wps\n",
      "KL is 1.1610900163650513\n",
      "0.211 perplexity: 1.210 speed: 68541 wps\n",
      "KL is 1.1770429611206055\n",
      "0.309 perplexity: 1.210 speed: 69501 wps\n",
      "KL is 1.171146035194397\n",
      "0.408 perplexity: 1.211 speed: 69474 wps\n",
      "KL is 1.1720097064971924\n",
      "0.507 perplexity: 1.212 speed: 65439 wps\n",
      "KL is 1.176002860069275\n",
      "0.606 perplexity: 1.212 speed: 66125 wps\n",
      "KL is 1.166757583618164\n",
      "0.705 perplexity: 1.211 speed: 66643 wps\n",
      "KL is 1.1727277040481567\n",
      "0.804 perplexity: 1.211 speed: 66818 wps\n",
      "KL is 1.1711831092834473\n",
      "0.902 perplexity: 1.210 speed: 67111 wps\n",
      "KL is 1.1660807132720947\n",
      "Epoch: 8 Train Perplexity: 1.210\n",
      "Epoch: 8 Valid Perplexity: 1.202\n",
      "Epoch: 9 Learning rate: 1.000\n",
      "0.000 perplexity: 1.129 speed: 68526 wps\n",
      "KL is 1.1661604642868042\n",
      "0.013 perplexity: 1.201 speed: 78822 wps\n",
      "KL is 1.1706616878509521\n",
      "0.112 perplexity: 1.205 speed: 73153 wps\n",
      "KL is 1.1633532047271729\n",
      "0.211 perplexity: 1.200 speed: 71648 wps\n",
      "KL is 1.1675487756729126\n",
      "0.309 perplexity: 1.205 speed: 71475 wps\n",
      "KL is 1.168290615081787\n",
      "0.408 perplexity: 1.207 speed: 71254 wps\n",
      "KL is 1.161577582359314\n",
      "0.507 perplexity: 1.208 speed: 71269 wps\n",
      "KL is 1.15355384349823\n",
      "0.606 perplexity: 1.208 speed: 71092 wps\n",
      "KL is 1.1687486171722412\n",
      "0.705 perplexity: 1.209 speed: 70970 wps\n",
      "KL is 1.1658681631088257\n",
      "0.804 perplexity: 1.208 speed: 70965 wps\n",
      "KL is 1.1652863025665283\n",
      "0.902 perplexity: 1.208 speed: 70837 wps\n",
      "KL is 1.1621791124343872\n",
      "Epoch: 9 Train Perplexity: 1.207\n",
      "Epoch: 9 Valid Perplexity: 1.183\n",
      "Epoch: 10 Learning rate: 1.000\n",
      "0.000 perplexity: 1.162 speed: 48257 wps\n",
      "KL is 1.159807562828064\n",
      "0.013 perplexity: 1.216 speed: 67920 wps\n",
      "KL is 1.1687610149383545\n",
      "0.112 perplexity: 1.209 speed: 71244 wps\n",
      "KL is 1.162287950515747\n",
      "0.211 perplexity: 1.209 speed: 71380 wps\n",
      "KL is 1.164023756980896\n",
      "0.309 perplexity: 1.211 speed: 71539 wps\n",
      "KL is 1.1643043756484985\n",
      "0.408 perplexity: 1.212 speed: 71485 wps\n",
      "KL is 1.167428970336914\n",
      "0.507 perplexity: 1.212 speed: 71425 wps\n",
      "KL is 1.1745524406433105\n",
      "0.606 perplexity: 1.210 speed: 71322 wps\n",
      "KL is 1.166652798652649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.705 perplexity: 1.211 speed: 71407 wps\n",
      "KL is 1.1743391752243042\n",
      "0.804 perplexity: 1.210 speed: 71340 wps\n",
      "KL is 1.1708248853683472\n",
      "0.902 perplexity: 1.210 speed: 71383 wps\n",
      "KL is 1.1756165027618408\n",
      "Epoch: 10 Train Perplexity: 1.210\n",
      "Epoch: 10 Valid Perplexity: 1.178\n",
      "Test Perplexity: 1.182\n",
      "Saving model to ./saved_model/.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Training !\n",
    "with tf.Graph().as_default():\n",
    "    tf.train.import_meta_graph(metagraph)\n",
    "    for model in models.values():\n",
    "        model.import_ops()\n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)\n",
    "    with sv.managed_session(config=config_proto) as session:\n",
    "\n",
    "        for i in range(train_config.max_max_epoch):\n",
    "            lr_decay = train_config.lr_decay ** max(i + 1 - train_config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, train_config.learning_rate * lr_decay)\n",
    "\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            train_perplexity = run_epoch(session, m, eval_op=m.train_op,\n",
    "                                         verbose=True)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            valid_perplexity = run_epoch(session, mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "            \n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "        \n",
    "\n",
    "        print(\"Saving model to %s.\" % save_path)\n",
    "        sv.saver.save(session, save_path, global_step=sv.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/-15380\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./saved_model/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 15380.\n",
      "Test Perplexity: 1.182\n",
      "----------------------------------------------------------------\n",
      "------------------ Prediction of Sentences ---------------------\n",
      "Computing batch 0/2500\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "print (\"Testing\")\n",
    "predicted = []   # Variable to store predictions\n",
    "with tf.Graph().as_default():\n",
    "    tf.train.import_meta_graph(metagraph)\n",
    "    for model in models.values():\n",
    "        model.import_ops()\n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)\n",
    "    with sv.managed_session(config=config_proto) as session:\n",
    "        \n",
    "       # session = tf.Session()\n",
    "    \n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "        print (\"----------------------------------------------------------------\")\n",
    "        print (\"------------------ Prediction of Sentences ---------------------\")\n",
    "\n",
    "       #  inputs, predicted = fetch_output(session, mtest)\n",
    "\n",
    "        costs = 0.0\n",
    "        state = session.run(model.initial_state)\n",
    "\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        targets = []\n",
    "        fetches = {\n",
    "            \"final_state\": model.final_state,\n",
    "            \"output\": model.output,\n",
    "            \"input\": model.input_data,\n",
    "            \"targets\": model.targets\n",
    "        }\n",
    "\n",
    "        for step in range(model.input.epoch_size):\n",
    "            feed_dict = {}\n",
    "            for i, (c, h) in enumerate(model.initial_state):\n",
    "                feed_dict[c] = state[i].c\n",
    "                feed_dict[h] = state[i].h\n",
    "\n",
    "            print (\"Computing batch %i/%i\"%(step, model.input.epoch_size))\n",
    "            vals = session.run(fetches, feed_dict)\n",
    "            state = vals[\"final_state\"]\n",
    "            output = vals[\"output\"]\n",
    "            input_i = vals[\"targets\"]\n",
    "            \n",
    "            outputs.append(output)\n",
    "            inputs.append(input_i)\n",
    "            targets.append(vals[\"input\"])\n",
    "            break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input and output of the first chain of the first batch\n",
      "[[-0.06079967]\n",
      " [ 0.08959871]\n",
      " [ 0.15604454]\n",
      " [ 0.13158283]\n",
      " [ 0.03252221]\n",
      " [-0.10924137]\n",
      " [-0.25686768]\n",
      " [-0.37872651]\n",
      " [-0.45631298]\n",
      " [-0.48690122]\n",
      " [-0.48119387]\n",
      " [-0.45587265]\n",
      " [-0.42512256]\n",
      " [-0.39432946]\n",
      " [-0.3595165 ]\n",
      " [-0.31174734]\n",
      " [-0.24445727]\n",
      " [-0.16009708]\n",
      " [-0.07210729]\n",
      " [-0.00054925]]\n",
      "[[  4.46430504e-01   5.53569555e-01]\n",
      " [  2.05668762e-01   7.94331253e-01]\n",
      " [  5.26724383e-02   9.47327554e-01]\n",
      " [  8.95784676e-01   1.04215331e-01]\n",
      " [  9.82564926e-01   1.74350571e-02]\n",
      " [  9.94961262e-01   5.03870891e-03]\n",
      " [  9.98609900e-01   1.39012304e-03]\n",
      " [  9.98778403e-01   1.22156646e-03]\n",
      " [  9.97399092e-01   2.60091806e-03]\n",
      " [  8.23118091e-01   1.76881880e-01]\n",
      " [  8.14129889e-01   1.85870111e-01]\n",
      " [  1.78001553e-01   8.21998477e-01]\n",
      " [  1.95171788e-01   8.04828227e-01]\n",
      " [  3.58592033e-01   6.41407967e-01]\n",
      " [  2.95210481e-02   9.70479012e-01]\n",
      " [  2.06252094e-03   9.97937560e-01]\n",
      " [  4.38064599e-04   9.99561965e-01]\n",
      " [  3.17424332e-04   9.99682546e-01]\n",
      " [  4.61713469e-04   9.99538302e-01]\n",
      " [  1.28196562e-02   9.87180293e-01]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Input and output of the first chain of the first batch\")\n",
    "print (inputs[0][0])\n",
    "print (outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "[1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "selected_words = np.argmax(outputs[0][0], axis = 1)\n",
    "print (outputs[0][0].shape)\n",
    "print (selected_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
